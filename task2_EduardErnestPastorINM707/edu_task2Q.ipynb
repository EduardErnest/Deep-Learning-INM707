{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-LEARNING IMPLEMENTATION ON A GRIDWORLD ENVIRONMENT --  INM707 taught by Michael Garcia Ortiz at City University 2021\n",
    "\n",
    "\"\"\"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.\n",
    "\n",
    " source https://en.wikipedia.org/wiki/Q-learning\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports used for task2 \n",
    "# import torch\n",
    "# is_cuda_available = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "# print(device)\n",
    "from typing import Tuple\n",
    "from q_maze import QMaze, Action\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from e_greedy_pol import E_greedy_policy\n",
    "import random as random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing action value functions using E_greedy_policy\n",
    "\n",
    "For this method we will use the E_greedy_policy to help us compute and estimate the **q-value** of each state.\n",
    "\n",
    "The **q-value** is the **mean** expected future reward following an action from a given state. Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly\n",
    "class E_greedy_policy:\n",
    "    def __init__(self, epsilon, decay):\n",
    "\n",
    "        self.epsilon = epsilon #initial value of epsilon\n",
    "        self.epsilon_start = epsilon\n",
    "        self.decay = decay #parameter used to control how much the agent should explore and exploit when using epislon-greedy policy.\n",
    "\n",
    "    # This function is used to select the action with max values \n",
    "    #For us to be able to select max values we need to know the state and the q_values\n",
    "    def __call__(self, state, q_values): \n",
    "\n",
    "        is_greedy = random.random() > self.epsilon\n",
    "\n",
    "        if is_greedy:\n",
    "            # we select a greedy action by getting the max q_values from the grid\n",
    "            action_index = np.argmax(q_values[state])\n",
    "        else:\n",
    "            # else we get a random choice from action\n",
    "            action_index = random.choice(list(Action)).value.index\n",
    "        #while selected_action = None\n",
    "        selected_action = None\n",
    "        #we pick an action from our possible action moves\n",
    "        for a in list(Action):\n",
    "            if a.value.index == action_index:\n",
    "                selected_action = a\n",
    "        return selected_action\n",
    "\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = self.epsilon * self.decay\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.epsilon = self.epsilon_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \"\"\"Instant diff parameters for calc Q\"\"\"\n",
    "    def __init__(self, policy, env, gamma, alpha):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.env = env.size\n",
    "        self.coord_to_index_state = env.coord_to_index_state\n",
    "\n",
    "        self.q_values = np.zeros( (self.env * self.env,(len(list(Action) ))))\n",
    "\n",
    "\n",
    "    #We are updating the values from our q.values table after each step\n",
    "    def update_values(self, state_current, state_next, action, reward):\n",
    "\n",
    "        old_value = self.q_values[state_current, action]\n",
    "        next_max = np.max(self.q_values[state_next])\n",
    "        \n",
    "        #Calculate the new q.values from the q-learning formula\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "        self.q_values[state_current, action] = new_value\n",
    "\n",
    "\n",
    "    #we are assigning the maximum qvalues to the grid in the maze so we can calculate the optimum route the agent must take in order to maximise reward\n",
    "    def new_values(self):\n",
    "        value_matrix = np.zeros( (self.env, self.env) )\n",
    "    \n",
    "        for i in range(self.env):\n",
    "            for j in range(self.env):\n",
    "\n",
    "                state = self.coord_to_index_state[i, j]\n",
    "                \n",
    "                value_matrix[i,j] = max(self.q_values[state])\n",
    "                \n",
    "        return value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X \nX . X X . X X . . . . . . A . . . . . X \nX . . . . . . . X . X X . X X . X X X X \nX . X . X X X X X X X X . . X . . . . X \nX X X . . . . . . . . X . X X X . X X X \nX . . . X . X . X X X X X X . X X X . X \nX . X X X . X . . . . . X X . . . . . X \nX . . . X . X X . X X . . . . X X X . X \nX . X . X . X X . . X . X X X X X X X X \nX . X X X . X . . X X . . . . . . X X X \nX . . X X . X X . . X X . X X X . . . X \nX . X X . . X X . X X X . . . X X . X X \nX . . X X . X X . . . X . X . . X . . X \nX . X X . . X X . X X X X X . X X X X X \nX . . X X . X X . . . . . X . . . X X X \nX X . X . . X X X . X X X X X X X X . X \nX X X X . X X . . . X X X . X . X X . X \nX . . . . X X X X . . . . . . . . X . X \nX . X X . . X X . . X X . X . X . . . X \nX X X X X X X X X X X X X X X X X X O X \n\n"
     ]
    }
   ],
   "source": [
    "maze = QMaze(20)\n",
    "maze.reset()\n",
    "maze.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it\n",
    "\n",
    "#### 1st set of experiments --> 0.9 gamma 0.1 alpha / Epsilon is 1 / Decay is 0.999\n",
    "#### 2nd set of experiments --> 0.8 gamme 00.1 alpha / Epsilon is 1 / Decay is 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting the parameters of the experiments\n",
    "epolicy = E_greedy_policy(1, 0.999)\n",
    "epolicy.reset()\n",
    "qlearning = Qlearning(epolicy,maze, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 0, epsilon: 0.999, reward: -11102\n",
      "Episode: 10, epsilon: 0.9890548353295385, reward: -27836\n",
      "Episode: 20, epsilon: 0.9792086759647052, reward: -54879\n",
      "Episode: 30, epsilon: 0.9694605362958227, reward: -21164\n",
      "Episode: 40, epsilon: 0.959809440525076, reward: -24358\n",
      "Episode: 50, epsilon: 0.9502544225688344, reward: -5242\n",
      "Episode: 60, epsilon: 0.9407945259609451, reward: -2815\n",
      "Episode: 70, epsilon: 0.9314288037569908, reward: -5643\n",
      "Episode: 80, epsilon: 0.9221563184394991, reward: -2936\n",
      "Episode: 90, epsilon: 0.9129761418240965, reward: -5040\n",
      "Episode: 100, epsilon: 0.9038873549665959, reward: -17670\n",
      "Episode: 110, epsilon: 0.8948890480710096, reward: -9106\n",
      "Episode: 120, epsilon: 0.8859803203984784, reward: -2642\n",
      "Episode: 130, epsilon: 0.8771602801771059, reward: -2077\n",
      "Episode: 140, epsilon: 0.8684280445126921, reward: -973\n",
      "Episode: 150, epsilon: 0.8597827393003539, reward: -1795\n",
      "Episode: 160, epsilon: 0.8512234991370281, reward: -1160\n",
      "Episode: 170, epsilon: 0.8427494672348417, reward: -1987\n",
      "Episode: 180, epsilon: 0.8343597953353479, reward: -2158\n",
      "Episode: 190, epsilon: 0.8260536436246144, reward: -190\n",
      "Episode: 200, epsilon: 0.8178301806491574, reward: -463\n",
      "Episode: 210, epsilon: 0.8096885832327116, reward: -1108\n",
      "Episode: 220, epsilon: 0.8016280363938307, reward: -1373\n",
      "Episode: 230, epsilon: 0.7936477332643059, reward: -797\n",
      "Episode: 240, epsilon: 0.7857468750083979, reward: -5273\n",
      "Episode: 250, epsilon: 0.7779246707428734, reward: -1932\n",
      "Episode: 260, epsilon: 0.7701803374578359, reward: -961\n",
      "Episode: 270, epsilon: 0.7625130999383466, reward: -845\n",
      "Episode: 280, epsilon: 0.7549221906868242, reward: -855\n",
      "Episode: 290, epsilon: 0.7474068498462175, reward: -649\n",
      "Episode: 300, epsilon: 0.7399663251239436, reward: -840\n",
      "Episode: 310, epsilon: 0.7325998717165821, reward: -208\n",
      "Episode: 320, epsilon: 0.7253067522353204, reward: -514\n",
      "Episode: 330, epsilon: 0.7180862366321393, reward: -480\n",
      "Episode: 340, epsilon: 0.7109376021267352, reward: -871\n",
      "Episode: 350, epsilon: 0.7038601331341691, reward: -31\n",
      "Episode: 360, epsilon: 0.6968531211932361, reward: -275\n",
      "Episode: 370, epsilon: 0.6899158648955466, reward: -371\n",
      "Episode: 380, epsilon: 0.6830476698153162, reward: -420\n",
      "Episode: 390, epsilon: 0.6762478484398523, reward: -139\n",
      "Episode: 400, epsilon: 0.6695157201007336, reward: -154\n",
      "Episode: 410, epsilon: 0.662850610905674, reward: -347\n",
      "Episode: 420, epsilon: 0.6562518536710664, reward: -181\n",
      "Episode: 430, epsilon: 0.6497187878551962, reward: -46\n",
      "Episode: 440, epsilon: 0.6432507594921204, reward: -6\n",
      "Episode: 450, epsilon: 0.6368471211262058, reward: -83\n",
      "Episode: 460, epsilon: 0.6305072317473174, reward: -285\n",
      "Episode: 470, epsilon: 0.6242304567266527, reward: -448\n",
      "Episode: 480, epsilon: 0.6180161677532153, reward: -153\n",
      "Episode: 490, epsilon: 0.6118637427709198, reward: -288\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 261.105625 248.518125\" width=\"261.105625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-11T13:32:06.667043</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 261.105625 248.518125 \nL 261.105625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 253.905625 224.64 \nL 253.905625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pdfe3af026a)\">\n    <image height=\"218\" id=\"imagef8dfe8fb07\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"36.465625\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1dc4e574ee\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"41.901625\" xlink:href=\"#m1dc4e574ee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(38.720375 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.261625\" xlink:href=\"#m1dc4e574ee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(93.080375 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"150.621625\" xlink:href=\"#m1dc4e574ee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(144.259125 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.981625\" xlink:href=\"#m1dc4e574ee\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(198.619125 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9b8afc06c2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"12.636\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.0 -->\n      <g transform=\"translate(13.5625 16.435219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"39.816\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2.5 -->\n      <g transform=\"translate(13.5625 43.615219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"66.996\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5.0 -->\n      <g transform=\"translate(13.5625 70.795219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"94.176\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 7.5 -->\n      <g transform=\"translate(13.5625 97.975219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"121.356\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10.0 -->\n      <g transform=\"translate(7.2 125.155219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"148.536\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 12.5 -->\n      <g transform=\"translate(7.2 152.335219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"175.716\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 15.0 -->\n      <g transform=\"translate(7.2 179.515219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9b8afc06c2\" y=\"202.896\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 17.5 -->\n      <g transform=\"translate(7.2 206.695219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 253.905625 224.64 \nL 253.905625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 253.905625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 253.905625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pdfe3af026a\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training time: 32.463346004486084s\n"
     ]
    }
   ],
   "source": [
    "# Train the agent 2000 times\n",
    "episodes = 500\n",
    "times_visited = np.zeros((qlearning.env * qlearning.env))\n",
    "start = time.time()\n",
    "for episode in range(episodes):\n",
    "    s = maze.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    while not done:\n",
    "        action = epolicy(s, qlearning.q_values)\n",
    "\n",
    "        # Get the next state and reward after performing the action\n",
    "        s_next, r, done = maze.step(action)\n",
    "        times_visited[s_next] += 1  \n",
    "        reward += r\n",
    "\n",
    "        # Update the values of the q table\n",
    "        qlearning.update_values(s, s_next, action.value.index, r)\n",
    "\n",
    "        s = s_next\n",
    "    \n",
    "    # Update the epsilon policy\n",
    "    epolicy.update_epsilon()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode}, epsilon: {epolicy.epsilon}, reward: {reward}\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Get the best actions for each state\n",
    "best_actions = qlearning.new_values()\n",
    "plt.imshow(best_actions)\n",
    "plt.show()\n",
    "# Print the time spent training the agent\n",
    "print(f\"Training time: {end-start}s\")"
   ]
  },
  {
   "source": [
    "### Testing and visualising the agent in the MAZE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X \nX . X X . X X . I . . . . . . . . . . X \nX . . . . . . . X . X X . X X . X X X X \nX . X . X X X X X X X X . . X . . . . X \nX X X . . . . . . . . X . X X X . X X X \nX . . . X . X . X X X X X X . X X X . X \nX . X X X . X . . . . . X X . . . . . X \nX . . . X . X X . X X . . . . X X X . X \nX . X . X . X X . . X . X X X X X X X X \nX . X X X . X . . X X . . . . . . X X X \nX . . X X . X X . . X X . X X X . . . X \nX . X X . . X X . X X X . . . X X . X X \nX . . X X . X X . . . X . X . . X . . X \nX . X X . . X X . X X X X X . X X X X X \nX . . X X . X X . . . . . X . . . X X X \nX X . X . . X X X . X X X X X X X X . X \nX X X X . X X . . . X X X . X . X X . X \nX . . . . X X X X . . . . . . . . X . X \nX . X X . . X X . . X X . X . X . . . X \nX X X X X X X X X X X X X X X X X X A X \n\nTime step: 128\nState: 380\nAction: Action.DOWN\nReward: 399\n"
     ]
    }
   ],
   "source": [
    "# Test the agent by visualizing it\n",
    "state = maze.reset()\n",
    "timestep, rewards = 0, 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    selected_action = epolicy(state, qlearning.q_values)\n",
    "    \n",
    "    # Perform the selected action\n",
    "    state, reward, done = maze.step(selected_action)\n",
    "    rewards += reward\n",
    "    \n",
    "    # Display the maze and action information\n",
    "    clear_output(wait=True)\n",
    "    maze.display()\n",
    "    print(f\"Time step: {timestep}\")\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Action: {selected_action}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "\n",
    "    timestep += 1"
   ]
  },
  {
   "source": [
    "### Evaluating the performance of the agent based on our policies "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results after 500 episodes:\nAverage timesteps per episode: 138.584\nAverage rewards per episode: -174.598\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the agent after training\n",
    "\n",
    "total_timesteps, total_penalties = 0, 0\n",
    "total_rewards = 0\n",
    "episodes = 500\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = maze.reset()\n",
    "    timesteps, reward = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        selected_action = epolicy(state, qlearning.q_values)\n",
    "        \n",
    "        # Perform selected action and get the next state and current reward\n",
    "        state, reward, done = maze.step(selected_action)\n",
    "        \n",
    "        # Keep track of the total reward\n",
    "        total_rewards += reward\n",
    "        \n",
    "        timesteps += 1\n",
    "\n",
    "    total_timesteps += timesteps\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_timesteps / episodes}\")\n",
    "print(f\"Average rewards per episode: {total_rewards / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0ec04c10a3ef611ea2836b5ff833f0501e0dcfc8e0500f516ccda9246622a0e8e",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}